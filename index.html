

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Hadoopy: Python wrapper for Hadoop using Cython &mdash; Hadoopy v.01 documentation</title>
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '.01',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="Hadoopy v.01 documentation" href="#" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li><a href="#">Hadoopy v.01 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="hadoopy-python-wrapper-for-hadoop-using-cython">
<h1>Hadoopy: Python wrapper for Hadoop using Cython<a class="headerlink" href="#hadoopy-python-wrapper-for-hadoop-using-cython" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
<ul class="simple">
</ul>
</div>
<p>Visit <a class="reference external" href="https://github.com/bwhite/hadoopy/">https://github.com/bwhite/hadoopy/</a> for the source.</p>
<p>Relevant blog posts</p>
<ul class="simple">
<li><a class="reference external" href="http://brandynwhite.com/hadoopy-cython-based-mapreduce-library-for-py">http://brandynwhite.com/hadoopy-cython-based-mapreduce-library-for-py</a></li>
</ul>
<div class="section" id="about">
<h2>About<a class="headerlink" href="#about" title="Permalink to this headline">¶</a></h2>
<p>Hadoopy is a Python wrapper for Hadoop Streaming written in Cython.  It is simple, fast, and readily hackable.  It has been tested on 700+ node clusters.  The goals of Hadoopy are</p>
<ul class="simple">
<li>Similar interface as the Hadoop API (design patterns usable between Python/Java interfaces)</li>
<li>General compatibility with dumbo to allow users to switch back and forth</li>
<li>Usable on Hadoop clusters without Python or admin access</li>
<li>Fast conversion and processing</li>
<li>Stay small and well documented</li>
<li>Be transparent with what is going on</li>
<li>Handle programs with complicated .so&#8217;s, ctypes, and extensions</li>
<li>Code written for hack-ability</li>
<li>Simple HDFS access (e.g., ls and cat)</li>
<li>Oozie support from the start</li>
<li>Protocol Buffers support (in progress)</li>
<li>Cython user code support (in progress)</li>
</ul>
</div>
<div class="section" id="benchmark">
<h2>Benchmark<a class="headerlink" href="#benchmark" title="Permalink to this headline">¶</a></h2>
<p>The majority of the time spent by Hadoopy (and Dumbo) is in the TypedBytes conversion code.  This is a simple binary serialization format that covers standard types with the ability to use Pickle for types not natively supported.  We generate a large set of test vectors (using the <a class="reference external" href="https://github.com/bwhite/hadoopy/blob/master/play/tb_make_test.py">tb_make_test.py</a> script), that have primatives, containers, and a uniform mix (GrabBag).  The idea is that by factoring out the types, we can easily see where optimization is needed.  Each element is read from stdin, then written to stdout.  Outside of the timing all of the values are compared to ensure that the final written values are the same.  Four methods are compared:  Hadoopy TypedBytes (<a class="reference external" href="https://github.com/bwhite/hadoopy/blob/master/play/speed_hadoopy.py">speed_hadoopy.py</a>), Hadoopy TypedBytes file interface (<a class="reference external" href="https://github.com/bwhite/hadoopy/blob/master/play/speed_hadoopyfp.py">speed_hadoopyfp.py</a>), <a class="reference external" href="https://github.com/klbostee/typedbytes">TypedBytes</a> (<a class="reference external" href="https://github.com/bwhite/hadoopy/blob/master/play/speed_tb.py">speed_tb.py</a>), and <a class="reference external" href="https://github.com/klbostee/ctypedbytes">cTypedBytes</a> (<a class="reference external" href="https://github.com/bwhite/hadoopy/blob/master/play/speed_tbc.py">speed_tbc.py</a>).  All columns are in seconds except for ratio.  The ratio is min(TB, cTB) / HadoopyFP (e.g., 7 means HadoopyFP is 7 times faster).</p>
<table border="1" class="docutils">
<colgroup>
<col width="27%" />
<col width="15%" />
<col width="15%" />
<col width="15%" />
<col width="15%" />
<col width="15%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Filename</th>
<th class="head">Hadoopy</th>
<th class="head">HadoopyFP</th>
<th class="head">TB</th>
<th class="head">cTB</th>
<th class="head">Ratio</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>double_100k.tb</td>
<td>0.148790</td>
<td>0.119961</td>
<td>0.904720</td>
<td>0.993845</td>
<td>7.541784</td>
</tr>
<tr><td>float_100k.tb</td>
<td>0.145637</td>
<td>0.118920</td>
<td>0.883124</td>
<td>0.992447</td>
<td>7.426198</td>
</tr>
<tr><td>gb_100k.tb</td>
<td>4.638573</td>
<td>4.011934</td>
<td>25.577765</td>
<td>16.515563</td>
<td>4.116609</td>
</tr>
<tr><td>bool_100k.tb</td>
<td>0.171327</td>
<td>0.150975</td>
<td>0.942188</td>
<td>0.542741</td>
<td>3.594907</td>
</tr>
<tr><td>dict_50.tb</td>
<td>0.394323</td>
<td>0.364878</td>
<td>1.649921</td>
<td>1.225979</td>
<td>3.359970</td>
</tr>
<tr><td>tuple_50.tb</td>
<td>0.370037</td>
<td>0.413579</td>
<td>1.546317</td>
<td>1.241491</td>
<td>3.001823</td>
</tr>
<tr><td>byte_100k.tb</td>
<td>0.183307</td>
<td>0.164549</td>
<td>0.894184</td>
<td>0.487520</td>
<td>2.962767</td>
</tr>
<tr><td>list_50.tb</td>
<td>0.355870</td>
<td>0.370738</td>
<td>1.529233</td>
<td>1.092422</td>
<td>2.946614</td>
</tr>
<tr><td>int_100k.tb</td>
<td>0.234842</td>
<td>0.193235</td>
<td>0.922423</td>
<td>0.526160</td>
<td>2.722903</td>
</tr>
<tr><td>long_100k.tb</td>
<td>0.761289</td>
<td>0.640638</td>
<td>1.727951</td>
<td>1.957162</td>
<td>2.697234</td>
</tr>
<tr><td>bytes_100_10k.tb</td>
<td>0.069889</td>
<td>0.069375</td>
<td>0.147470</td>
<td>0.096838</td>
<td>1.395862</td>
</tr>
<tr><td>string_100_10k.tb</td>
<td>0.106642</td>
<td>0.104784</td>
<td>0.157907</td>
<td>0.106571</td>
<td>1.017054</td>
</tr>
<tr><td>string_10k_10k.tb</td>
<td>6.392013</td>
<td>6.527343</td>
<td>6.494607</td>
<td>6.949912</td>
<td>0.994985</td>
</tr>
<tr><td>bytes_10k_10k.tb</td>
<td>3.073718</td>
<td>3.123196</td>
<td>3.039668</td>
<td>3.100858</td>
<td>0.973256</td>
</tr>
<tr><td>string_1k_10k.tb</td>
<td>0.742198</td>
<td>0.719119</td>
<td>0.686382</td>
<td>0.676537</td>
<td>0.940786</td>
</tr>
<tr><td>bytes_1k_10k.tb</td>
<td>0.379785</td>
<td>0.370314</td>
<td>0.329728</td>
<td>0.339387</td>
<td>0.890401</td>
</tr>
<tr><td>gb_single.tb</td>
<td>0.045760</td>
<td>0.042701</td>
<td>0.038656</td>
<td>0.034925</td>
<td>0.817896</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="example-hello-wordcount">
<h2>Example - Hello Wordcount!<a class="headerlink" href="#example-hello-wordcount" title="Permalink to this headline">¶</a></h2>
<p>Python Source (fully documented version in <a class="reference external" href="https://github.com/bwhite/hadoopy/blob/master/tests/wc.py">wc.py</a>)</p>
<div class="highlight-python"><div class="highlight"><pre><span class="sd">&quot;&quot;&quot;Hadoopy Wordcount Demo&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">hadoopy</span>

<span class="k">def</span> <span class="nf">mapper</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">value</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">word</span><span class="p">,</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">reducer</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
    <span class="n">accum</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">values</span><span class="p">:</span>
        <span class="n">accum</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
    <span class="k">yield</span> <span class="n">key</span><span class="p">,</span> <span class="n">accum</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">hadoopy</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">mapper</span><span class="p">,</span> <span class="n">reducer</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="n">__doc__</span><span class="p">)</span>
</pre></div>
</div>
<p>Command line test (run without args, it prints the docstring and quits because of doc=__doc__)</p>
<div class="highlight-python"><pre>$ python wc.py
Hadoopy Wordcount Demo</pre>
</div>
<p>Command line test (map)</p>
<div class="highlight-python"><pre>$ echo "a b a a b c" | python wc.py map
a    1
b    1
a    1
a    1
b    1
c    1</pre>
</div>
<p>Command line test (map/sort)</p>
<div class="highlight-python"><pre>$ echo "a b a a b c" | python wc.py map | sort
a    1
a    1
a    1
b    1
b    1
c    1</pre>
</div>
<p>Command line test (map/sort/reduce)</p>
<div class="highlight-python"><pre>$ echo "a b a a b c" | python wc.py map | sort | python wc.py reduce
a    3
b    2
c    1</pre>
</div>
<p>Here are a few test files</p>
<div class="highlight-python"><pre>$ hadoop fs -ls playground/
Found 3 items
-rw-r--r--   2 brandyn supergroup     259835 2011-01-17 18:56 /user/brandyn/playground/wc-input-alice.tb
-rw-r--r--   2 brandyn supergroup     167529 2011-01-17 18:56 /user/brandyn/playground/wc-input-alice.txt
-rw-r--r--   2 brandyn supergroup      60638 2011-01-17 18:56 /user/brandyn/playground/wc-input-alice.txt.gz</pre>
</div>
<p>We can also do this in Python</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">hadoopy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hadoopy</span><span class="o">.</span><span class="n">ls</span><span class="p">(</span><span class="s">&#39;playground/&#39;</span><span class="p">)</span>
<span class="go">[&#39;/user/brandyn/playground/wc-input-alice.tb&#39;, &#39;/user/brandyn/playground/wc-input-alice.txt&#39;, &#39;/user/brandyn/playground/wc-input-alice.txt.gz&#39;]</span>
</pre></div>
</div>
<p>Lets put wc-input-alice.txt through the word counter using Hadoop.  Each node in the cluster has Hadoopy installed (later we will show that it isn&#8217;t necessary with launch_frozen).  Note that it is using typedbytes, SequenceFiles, and the AutoInputFormat by default.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">cmd</span> <span class="o">=</span> <span class="n">hadoopy</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="s">&#39;playground/wc-input-alice.txt&#39;</span><span class="p">,</span> <span class="s">&#39;playground/out/&#39;</span><span class="p">,</span> <span class="s">&#39;wc.py&#39;</span><span class="p">)</span>
<span class="go">HadooPY: Running[hadoop jar /usr/lib/hadoop-0.20/contrib/streaming/hadoop-streaming-0.20.2+737.jar -output playground/out/ -input playground/wc-input-alice.txt -mapper &quot;python wc.py map&quot; -reducer &quot;python wc.py reduce&quot; -file wc.py -jobconf mapred.job.name=python wc.py -io typedbytes -outputformat org.apache.hadoop.mapred.SequenceFileOutputFormat -    inputformat AutoInputFormat]</span>
<span class="go">11/01/17 20:22:31 WARN streaming.StreamJob: -jobconf option is deprecated, please use -D instead.</span>
<span class="go">packageJobJar: [wc.py, /var/lib/hadoop-0.20/cache/brandyn/hadoop-unjar464849802654976085/] [] /tmp/streamjob1822202887260165136.jar tmpDir=null</span>
<span class="go">11/01/17 20:22:32 INFO mapred.FileInputFormat: Total input paths to process : 1</span>
<span class="go">11/01/17 20:22:32 INFO streaming.StreamJob: getLocalDirs(): [/var/lib/hadoop-0.20/cache/brandyn/mapred/local]</span>
<span class="go">11/01/17 20:22:32 INFO streaming.StreamJob: Running job: job_201101141644_0723</span>
<span class="go">11/01/17 20:22:32 INFO streaming.StreamJob: To kill this job, run:</span>
<span class="go">11/01/17 20:22:32 INFO streaming.StreamJob: /usr/lib/hadoop-0.20/bin/hadoop job  -Dmapred.job.tracker=umiacs.umd.edu:8021 -kill job_201101141644_0723</span>
<span class="go">11/01/17 20:22:32 INFO streaming.StreamJob: Tracking URL: http://umiacs.umd.edu:50030/jobdetails.jsp?jobid=job_201101141644_0723</span>
<span class="go">11/01/17 20:22:33 INFO streaming.StreamJob:  map 0%  reduce 0%</span>
<span class="go">11/01/17 20:22:40 INFO streaming.StreamJob:  map 50%  reduce 0%</span>
<span class="go">11/01/17 20:22:41 INFO streaming.StreamJob:  map 100%  reduce 0%</span>
<span class="go">11/01/17 20:22:52 INFO streaming.StreamJob:  map 100%  reduce 100%</span>
<span class="go">11/01/17 20:22:55 INFO streaming.StreamJob: Job complete: job_201101141644_0723</span>
<span class="go">11/01/17 20:22:55 INFO streaming.StreamJob: Output: playground/out/</span>
</pre></div>
</div>
<p>Return value is the command used</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span>
<span class="go">hadoop jar /usr/lib/hadoop-0.20/contrib/streaming/hadoop-streaming-0.20.2+737.jar -output playground/out/ -input playground/wc-input-alice.txt -mapper &quot;python wc.py map&quot; -reducer &quot;python wc.py reduce&quot; -file wc.py -jobconf mapred.job.name=python wc.py -io typedbytes -outputformat org.apache.hadoop.mapred.SequenceFileOutputFormat -inputformat AutoInputFormat</span>
</pre></div>
</div>
<p>Lets see what the output looks like.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">hadoopy</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="s">&#39;playground/out&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
<span class="go">[(&#39;*&#39;, 60), (&#39;-&#39;, 7), (&#39;3&#39;, 2), (&#39;4&#39;, 1), (&#39;A&#39;, 8), (&#39;I&#39;, 260), (&#39;O&#39;, 1), (&#39;a&#39;, 662), (&#39;&quot;I&#39;, 7), (&quot;&#39;A&quot;, 9)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="nb">cmp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span>
<span class="go">[(&#39;was&#39;, 329), (&#39;it&#39;, 356), (&#39;in&#39;, 401), (&#39;said&#39;, 416), (&#39;she&#39;, 484), (&#39;of&#39;, 596), (&#39;a&#39;, 662), (&#39;to&#39;, 773), (&#39;and&#39;, 780), (&#39;the&#39;, 1664)]</span>
</pre></div>
</div>
<p>Note that the output is stored in SequenceFiles and each key/value is stored encoded as TypedBytes, by using cat you don&#8217;t have to worry about any of that (if the output was compressed it would also be decompressed here).  This can also be done inside of a job for getting additional side-data off of HDFS.</p>
<p>What if we don&#8217;t want to install python, numpy, scipy, or your-custom-code-that-always-changes on every node in the cluster?  We have you covered there too.  I&#8217;ll remove hadoopy from all nodes except for the one executing the job.</p>
<div class="highlight-python"><pre>$ sudo rm -r /usr/local/lib/python2.6/dist-packages/hadoopy*</pre>
</div>
<p>Now it&#8217;s gone</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">hadoopy</span>
<span class="gt">Traceback (most recent call last):</span>
  File <span class="nb">&quot;&lt;stdin&gt;&quot;</span>, line <span class="m">1</span>, in <span class="n-Identifier">&lt;module&gt;</span>
<span class="nc">ImportError</span>: <span class="n-Identifier">No module named hadoopy</span>
</pre></div>
</div>
<p>The rest of the nodes were cleaned up the same way.  We modify the command, note that we now get the output from freeze at the top</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">cmd</span> <span class="o">=</span> <span class="n">hadoopy</span><span class="o">.</span><span class="n">launch_frozen</span><span class="p">(</span><span class="s">&#39;playground/wc-input-alice.txt&#39;</span><span class="p">,</span> <span class="s">&#39;playground/out_frozen/&#39;</span><span class="p">,</span> <span class="s">&#39;wc.py&#39;</span><span class="p">)</span>
<span class="go">Missing modules:</span>
<span class="go">? _md5 imported from hashlib</span>
<span class="go">? _scproxy imported from urllib</span>
<span class="go">? _sha imported from hashlib</span>
<span class="go">? _sha256 imported from hashlib</span>
<span class="go">? _sha512 imported from hashlib</span>

<span class="go">HadooPY: Running[hadoop jar /usr/lib/hadoop-0.20/contrib/streaming/hadoop-streaming-0.20.2+737.jar -output playground/out_frozen/ -input playground/wc-input-alice.txt -mapper &quot;wc map&quot; -reducer &quot;wc reduce&quot; -file frozen/_codecs_tw.so -file frozen/_codecs_cn.so -file frozen/sgmlop.so -file frozen/_codecs_iso2022.so -file frozen/_main.so -file frozen/_ssl.so -file frozen/_codecs_hk.so -file frozen/_codecs_jp.so -file frozen/_multibytecodec.so -file frozen/datetime.so -file frozen/_codecs_kr.so -file frozen/mmap.so -file frozen/readline.so -file frozen/_heapq.so -file frozen/bz2.so -file frozen/_typedbytes.so -file frozen/_ctypes.so -file frozen/_hashlib.so -file frozen/_multiprocessing.so -file frozen/pyexpat.so -file frozen/libpython2.6.so.1.0 -file frozen/termios.so -file frozen/wc -jobconf mapred.job.name=wc -io typedbytes -outputformat org.apache.hadoop.mapred.SequenceFileOutputFormat -inputformat AutoInputFormat]</span>
<span class="go">11/01/17 20:55:00 WARN streaming.StreamJob: -jobconf option is deprecated, please use -D instead.</span>
<span class="go">packageJobJar: [frozen/_codecs_tw.so, frozen/_codecs_cn.so, frozen/sgmlop.so, frozen/_codecs_iso2022.so, frozen/_main.so, frozen/_ssl.so, frozen/_codecs_hk.so, frozen/_codecs_jp.so, frozen/_multibytecodec.so, frozen/datetime.so, frozen/_codecs_kr.so, frozen/mmap.so, frozen/readline.so, frozen/_heapq.so, frozen/bz2.so, frozen/_typedbytes.so, frozen/_ctypes.so, frozen/_hashlib.so, frozen/_multiprocessing.so, frozen/pyexpat.so, frozen/libpython2.6.so.1.0, frozen/termios.so, frozen/wc, /var/lib/hadoop-0.20/cache/brandyn/hadoop-unjar6437825264052222661/] [] /tmp/streamjob9089438158340520087.jar tmpDir=null</span>
<span class="go">11/01/17 20:55:02 INFO mapred.FileInputFormat: Total input paths to process : 1</span>
<span class="go">11/01/17 20:55:02 INFO streaming.StreamJob: getLocalDirs(): [/var/lib/hadoop-0.20/cache/brandyn/mapred/local]</span>
<span class="go">11/01/17 20:55:02 INFO streaming.StreamJob: Running job: job_201101141644_0724</span>
<span class="go">11/01/17 20:55:02 INFO streaming.StreamJob: To kill this job, run:</span>
<span class="go">11/01/17 20:55:02 INFO streaming.StreamJob: /usr/lib/hadoop-0.20/bin/hadoop job  -Dmapred.job.tracker=umiacs.umd.edu:8021 -kill job_201101141644_0724</span>
<span class="go">11/01/17 20:55:02 INFO streaming.StreamJob: Tracking URL: http://umiacs.umd.edu:50030/jobdetails.jsp?jobid=job_201101141644_0724</span>
<span class="go">11/01/17 20:55:03 INFO streaming.StreamJob:  map 0%  reduce 0%</span>
<span class="go">11/01/17 20:55:09 INFO streaming.StreamJob:  map 50%  reduce 0%</span>
<span class="go">11/01/17 20:55:11 INFO streaming.StreamJob:  map 100%  reduce 0%</span>
<span class="go">11/01/17 20:55:21 INFO streaming.StreamJob:  map 100%  reduce 100%</span>
<span class="go">11/01/17 20:55:24 INFO streaming.StreamJob: Job complete: job_201101141644_0724</span>
<span class="go">11/01/17 20:55:24 INFO streaming.StreamJob: Output: playground/out_frozen/</span>
</pre></div>
</div>
<p>And lets check the output</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">hadoopy</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="s">&#39;playground/out_frozen&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
<span class="go">[(&#39;*&#39;, 60), (&#39;-&#39;, 7), (&#39;3&#39;, 2), (&#39;4&#39;, 1), (&#39;A&#39;, 8), (&#39;I&#39;, 260), (&#39;O&#39;, 1), (&#39;a&#39;, 662), (&#39;&quot;I&#39;, 7), (&quot;&#39;A&quot;, 9)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="nb">cmp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span>
<span class="go">[(&#39;was&#39;, 329), (&#39;it&#39;, 356), (&#39;in&#39;, 401), (&#39;said&#39;, 416), (&#39;she&#39;, 484), (&#39;of&#39;, 596), (&#39;a&#39;, 662), (&#39;to&#39;, 773), (&#39;and&#39;, 780), (&#39;the&#39;, 1664)]</span>
</pre></div>
</div>
<p>We can also generate a tar of the frozen script (useful when working with Oozie).  Note the &#8216;wc&#8217; is not wc.py, it is actually a self contained executable.</p>
<div class="highlight-python"><pre>$ python wc.py freeze wc.tar.gz
Missing modules:
? _md5 imported from hashlib
? _scproxy imported from urllib
? _sha imported from hashlib
? _sha256 imported from hashlib
? _sha512 imported from hashlib
$ tar -tzf wc.tar.gz
_codecs_tw.so
_codecs_cn.so
sgmlop.so
_codecs_iso2022.so
_main.so
_codecs_hk.so
_codecs_jp.so
_multibytecodec.so
datetime.so
_codecs_kr.so
mmap.so
readline.so
_heapq.so
bz2.so
_typedbytes.so
_ctypes.so
_multiprocessing.so
pyexpat.so
libpython2.6.so.1.0
termios.so
wc</pre>
</div>
<p>Lets open it up and try it out</p>
<div class="highlight-python"><pre>$ tar -xzf wc.py
$ ./wc
Hadoopy Wordcount Demo
$ python wc.py
Hadoopy Wordcount Demo
$ hexdump -C wc | head -n5
00000000  7f 45 4c 46 02 01 01 00  00 00 00 00 00 00 00 00  |.ELF............|
00000010  02 00 3e 00 01 00 00 00  80 41 40 00 00 00 00 00  |..&gt;......A@.....|
00000020  40 00 00 00 00 00 00 00  50 04 16 00 00 00 00 00  |@.......P.......|
00000030  00 00 00 00 40 00 38 00  09 00 40 00 1d 00 1c 00  |....@.8...@.....|
00000040  06 00 00 00 05 00 00 00  40 00 00 00 00 00 00 00  |........@.......|</pre>
</div>
<p>That&#8217;s a quick tour of Hadoopy.</p>
</div>
<div class="section" id="api">
<h2>API<a class="headerlink" href="#api" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="hadoopy.run">
<tt class="descclassname">hadoopy.</tt><tt class="descname">run</tt><big>(</big><em>mapper=None</em><span class="optional">[</span>, <em>reducer=None</em>, <em>combiner=None</em>, <em>**kw</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#hadoopy.run" title="Permalink to this definition">¶</a></dt>
<dd><p>This is to be called in all Hadoopy job&#8217;s.  Handles arguments passed in, calls the provided functions with input, and stores the output.</p>
<p>TypedBytes are used if (os.environ[&#8216;stream_map_input&#8217;] == &#8216;typedbytes&#8217;) which is a jobconf variable set in all jobs Hadoop is using TypedBytes with.</p>
<p>It is <em>highly</em> recommended that TypedBytes be used for all non-trivial tasks.  Keep in mind that the semantics of what you can safely emit from your functions is limited when using Text (i.e., no \t or \n).  You can use the base64 module to ensure that your output is clean.</p>
<div class="line-block">
<div class="line"><strong>Command Interface</strong></div>
<div class="line">The command line switches added to your script (e.g., script.py) are</div>
</div>
<dl class="docutils">
<dt>python script.py <em>map</em></dt>
<dd>Use the provided mapper</dd>
<dt>python script.py <em>reduce</em></dt>
<dd>Use the provided reducer</dd>
<dt>python script.py <em>combine</em></dt>
<dd>Use the provided combiner</dd>
<dt>python script.py <em>freeze</em> &lt;tar_path&gt; &lt;-Zadd_file0 -Zadd_file1...&gt;</dt>
<dd>Freeze the script to a tar file specified by &lt;tar_path&gt;.  The extension may be .tar or .tar.gz.  All files are placed in the root of the tar. Files specified with -Z will be added to the tar root.</dd>
</dl>
<div class="line-block">
<div class="line"><strong>Specification of mapper/reducer/combiner</strong></div>
<div class="line">Input Key/Value Types</div>
<div class="line-block">
<div class="line">For TypedBytes/SequenceFileInputFormat, the Key/Value are the decoded TypedBytes</div>
<div class="line">For TextInputFormat, the Key is a byte offset (int) and the Value is a line without the newline (string)</div>
<div class="line"><br /></div>
</div>
<div class="line">Output Key/Value Types</div>
<div class="line-block">
<div class="line">For TypedBytes, anything Pickle-able can be used</div>
<div class="line">For Text, types are converted to string.  Note that neither may contain \t or \n as these are used in the encoding.  Output is key\tvalue\n</div>
<div class="line"><br /></div>
</div>
<div class="line">Expected arguments</div>
<div class="line-block">
<div class="line">mapper(key, value) or mapper.map(key, value)</div>
<div class="line">reducer(key, values) or reducer.reduce(key, values)</div>
<div class="line">combiner(key, values) or combiner.reduce(key, values)</div>
<div class="line"><br /></div>
</div>
<div class="line">Optional methods</div>
<div class="line-block">
<div class="line">func.configure(): Called before any input read.  Returns None.</div>
<div class="line">func.close():  Called after all input read.  Returns None or Iterator of (key, value)</div>
<div class="line"><br /></div>
</div>
<div class="line">Expected return</div>
<div class="line-block">
<div class="line">None or Iterator of (key, value)</div>
</div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>mapper</strong> &#8211; Function or class following the above spec</li>
<li><strong>reducer</strong> &#8211; Function or class following the above spec</li>
<li><strong>combiner</strong> &#8211; Function or class following the above spec</li>
<li><strong>doc</strong> &#8211; If specified, on error print this and call sys.exit(1)</li>
</ul>
</td>
</tr>
<tr class="field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">True on error, else False (may not return if doc is set and
there is an error)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="hadoopy.status">
<tt class="descclassname">hadoopy.</tt><tt class="descname">status</tt><big>(</big><em>msg</em><span class="optional">[</span>, <em>err=None</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#hadoopy.status" title="Permalink to this definition">¶</a></dt>
<dd><p>Output a status message that is displayed in the Hadoop web interface</p>
<p>The status message will replace any other, if you want to append you must
do this yourself.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>msg</strong> &#8211; String representing the status message</li>
<li><strong>err</strong> &#8211; Func that outputs a string, if None then sys.stderr.write is used (default None)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="hadoopy.counter">
<tt class="descclassname">hadoopy.</tt><tt class="descname">counter</tt><big>(</big><em>group</em>, <em>counter</em><span class="optional">[</span>, <em>amount=1</em>, <em>err=None</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#hadoopy.counter" title="Permalink to this definition">¶</a></dt>
<dd><p>Output a counter update that is displayed in the Hadoop web interface</p>
<p>Counters are useful for quickly identifying the number of times an error
occurred, current progress, or coarse statistics.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>group</strong> &#8211; Counter group</li>
<li><strong>counter</strong> &#8211; Counter name</li>
<li><strong>amount</strong> &#8211; Value to add (default 1)</li>
<li><strong>err</strong> &#8211; Func that outputs a string, if None then sys.stderr.write is used (default None)</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="hadoopy.launch">
<tt class="descclassname">hadoopy.</tt><tt class="descname">launch</tt><big>(</big><em>in_name</em>, <em>out_name</em>, <em>script_path</em><span class="optional">[</span>, <em>mapper=True</em>, <em>reducer=True</em>, <em>combiner=False</em>, <em>partitioner=False</em>, <em>files=()</em>, <em>jobconfs=()</em>, <em>cmdenvs=()</em>, <em>copy_script=True</em>, <em>hstreaming=None</em>, <em>name=None</em>, <em>use_typedbytes=True</em>, <em>use_seqoutput=True</em>, <em>use_autoinput=True</em>, <em>pretend=False</em>, <em>add_python=True</em>, <em>config=None</em>, <em>**kw</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#hadoopy.launch" title="Permalink to this definition">¶</a></dt>
<dd><p>Run Hadoop given the parameters</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_name</strong> &#8211; Input path (string or list)</li>
<li><strong>out_name</strong> &#8211; Output path</li>
<li><strong>script_path</strong> &#8211; Path to the script (e.g., script.py)</li>
<li><strong>mapper</strong> &#8211; If True, the mapper is &#8220;script.py map&#8221;.  If string, the mapper is the value</li>
<li><strong>reducer</strong> &#8211; If True (default), the reducer is &#8220;script.py reduce&#8221;.  If string, the reducer is the value</li>
<li><strong>combiner</strong> &#8211; If True, the combiner is &#8220;script.py combine&#8221; (default False).  If string, the combiner is the value</li>
<li><strong>partitioner</strong> &#8211; If True, the partitioner is the value.</li>
<li><strong>copy_script</strong> &#8211; If True, the script is added to the files list.</li>
<li><strong>files</strong> &#8211; Extra files (other than the script) (string or list).  NOTE: Hadoop copies the files into working directory</li>
<li><strong>jobconfs</strong> &#8211; Extra jobconf parameters (string or list)</li>
<li><strong>cmdenvs</strong> &#8211; Extra cmdenv parameters (string or list)</li>
<li><strong>hstreaming</strong> &#8211; The full hadoop streaming path to call.</li>
<li><strong>use_typedbytes</strong> &#8211; If True (default), use typedbytes IO.</li>
<li><strong>use_seqoutput</strong> &#8211; True (default), output sequence file. If False, output is text.</li>
<li><strong>use_autoinput</strong> &#8211; If True (default), sets the input format to auto.</li>
<li><strong>pretend</strong> &#8211; If true, only build the command and return.</li>
<li><strong>add_python</strong> &#8211; If true, use &#8216;python script_name.py&#8217;</li>
<li><strong>config</strong> &#8211; If a string, set the hadoop config path</li>
</ul>
</td>
</tr>
<tr class="field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">The hadoop command called.</p>
</td>
</tr>
<tr class="field"><th class="field-name">Raises :</th><td class="field-body"><p class="first">subprocess.CalledProcessError: Hadoop error.</p>
</td>
</tr>
<tr class="field"><th class="field-name">Raises :</th><td class="field-body"><p class="first last">OSError: Hadoop streaming not found.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="hadoopy.launch_frozen">
<tt class="descclassname">hadoopy.</tt><tt class="descname">launch_frozen</tt><big>(</big><em>in_name</em>, <em>out_name</em>, <em>script_path</em><span class="optional">[</span>, <em>mapper=True</em>, <em>reducer=True</em>, <em>combiner=False</em>, <em>partitioner=False</em>, <em>files=()</em>, <em>jobconfs=()</em>, <em>cmdenvs=()</em>, <em>copy_script=True</em>, <em>hstreaming=None</em>, <em>name=None</em>, <em>use_typedbytes=True</em>, <em>use_seqoutput=True</em>, <em>use_autoinput=True</em>, <em>pretend=False</em>, <em>add_python=True</em>, <em>config=None</em>, <em>verbose=False</em>, <em>**kw</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#hadoopy.launch_frozen" title="Permalink to this definition">¶</a></dt>
<dd><p>Freezes a script and then launches it.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_name</strong> &#8211; Input path (string or list)</li>
<li><strong>out_name</strong> &#8211; Output path</li>
<li><strong>script_path</strong> &#8211; Path to the script (e.g., script.py)</li>
<li><strong>mapper</strong> &#8211; If True, the mapper is &#8220;script.py map&#8221;.  If string, the mapper is the value</li>
<li><strong>reducer</strong> &#8211; If True (default), the reducer is &#8220;script.py reduce&#8221;.  If string, the reducer is the value</li>
<li><strong>combiner</strong> &#8211; If True, the combiner is &#8220;script.py combine&#8221; (default False).  If string, the combiner is the value</li>
<li><strong>partitioner</strong> &#8211; If True, the partitioner is the value.</li>
<li><strong>copy_script</strong> &#8211; If True, the script is added to the files list.</li>
<li><strong>files</strong> &#8211; Extra files (other than the script) (string or list).  NOTE: Hadoop copies the files into working directory</li>
<li><strong>jobconfs</strong> &#8211; Extra jobconf parameters (string or list)</li>
<li><strong>cmdenvs</strong> &#8211; Extra cmdenv parameters (string or list)</li>
<li><strong>hstreaming</strong> &#8211; The full hadoop streaming path to call.</li>
<li><strong>use_typedbytes</strong> &#8211; If True (default), use typedbytes IO.</li>
<li><strong>use_seqoutput</strong> &#8211; True (default), output sequence file. If False, output is text.</li>
<li><strong>use_autoinput</strong> &#8211; If True (default), sets the input format to auto.</li>
<li><strong>pretend</strong> &#8211; If true, only build the command and return.</li>
<li><strong>add_python</strong> &#8211; If true, use &#8216;python script_name.py&#8217;</li>
<li><strong>config</strong> &#8211; If a string, set the hadoop config path</li>
<li><strong>verbose</strong> &#8211; If true, output to stdout all command results.</li>
</ul>
</td>
</tr>
<tr class="field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">The hadoop command called.</p>
</td>
</tr>
<tr class="field"><th class="field-name">Raises :</th><td class="field-body"><p class="first">subprocess.CalledProcessError: Hadoop or freeze error.</p>
</td>
</tr>
<tr class="field"><th class="field-name">Raises :</th><td class="field-body"><p class="first last">OSError: Hadoop streaming or freeze not found.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="hadoopy.ls">
<tt class="descclassname">hadoopy.</tt><tt class="descname">ls</tt><big>(</big><em>path</em><big>)</big><a class="headerlink" href="#hadoopy.ls" title="Permalink to this definition">¶</a></dt>
<dd><p>List files on HDFS.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path</strong> &#8211; A string (potentially with wildcards).</td>
</tr>
<tr class="field"><th class="field-name">Return type:</th><td class="field-body">A list of strings representing HDFS paths.</td>
</tr>
<tr class="field"><th class="field-name">Raises :</th><td class="field-body">IOError: An error occurred listing the directory (e.g., not available).</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="hadoopy.cat">
<tt class="descclassname">hadoopy.</tt><tt class="descname">cat</tt><big>(</big><em>path</em><span class="optional">[</span>, <em>procs=10</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#hadoopy.cat" title="Permalink to this definition">¶</a></dt>
<dd><p>Read typedbytes sequence files on HDFS (with optional compression).</p>
<dl class="docutils">
<dt>Args:</dt>
<dd>path: A string (potentially with wildcards).
procs: Number of processes to use.</dd>
<dt>Returns:</dt>
<dd>An iterator of key, value pairs.</dd>
<dt>Raises:</dt>
<dd>IOError: An error occurred listing the directory (e.g., not available).</dd>
</dl>
<p>Read typedbytes sequence files on HDFS (with optional compression).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>path</strong> &#8211; A string (potentially with wildcards).</li>
<li><strong>procs</strong> &#8211; Number of processes to use.</li>
</ul>
</td>
</tr>
<tr class="field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">An iterator of key, value pairs.</p>
</td>
</tr>
<tr class="field"><th class="field-name">Raises :</th><td class="field-body"><p class="first last">IOError: An error occurred listing the directory (e.g., not available).</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="hadoopy.GroupedValues">
<em class="property">class </em><tt class="descclassname">hadoopy.</tt><tt class="descname">GroupedValues</tt><a class="headerlink" href="#hadoopy.GroupedValues" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="class">
<dt id="hadoopy.Test">
<em class="property">class </em><tt class="descclassname">hadoopy.</tt><tt class="descname">Test</tt><big>(</big><em>*args</em>, <em>**kw</em><big>)</big><a class="headerlink" href="#hadoopy.Test" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="hadoopy.Test.groupby_kv">
<tt class="descname">groupby_kv</tt><big>(</big><em>kv</em><big>)</big><a class="headerlink" href="#hadoopy.Test.groupby_kv" title="Permalink to this definition">¶</a></dt>
<dd><p>Group sorted KeyValue pairs</p>
<dl class="docutils">
<dt>Args:</dt>
<dd>kv: Iterator of KeyValue pairs</dd>
<dt>Returns:</dt>
<dd>Grouped KeyValue pairs in sorted order</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="hadoopy.Test.shuffle_kv">
<tt class="descname">shuffle_kv</tt><big>(</big><em>kv</em><big>)</big><a class="headerlink" href="#hadoopy.Test.shuffle_kv" title="Permalink to this definition">¶</a></dt>
<dd><p>Given KeyValue pairs, sort, then group</p>
<dl class="docutils">
<dt>Args:</dt>
<dd>kv: Iterator of KeyValue pairs</dd>
<dt>Returns:</dt>
<dd>Grouped KeyValue pairs in sorted order</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="hadoopy.Test.sort_kv">
<tt class="descname">sort_kv</tt><big>(</big><em>kv</em><big>)</big><a class="headerlink" href="#hadoopy.Test.sort_kv" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform a stable sort on KeyValue pair keys</p>
<dl class="docutils">
<dt>Args:</dt>
<dd>kv: Iterator of KeyValue pairs</dd>
<dt>Returns:</dt>
<dd>Grouped KeyValue pairs in sorted order</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="hadoopy.TypedBytesFile">
<em class="property">class </em><tt class="descclassname">hadoopy.</tt><tt class="descname">TypedBytesFile</tt><a class="headerlink" href="#hadoopy.TypedBytesFile" title="Permalink to this definition">¶</a></dt>
<dd><dl class="attribute">
<dt id="hadoopy.TypedBytesFile.next">
<tt class="descname">next</tt><a class="headerlink" href="#hadoopy.TypedBytesFile.next" title="Permalink to this definition">¶</a></dt>
<dd><p>x.next() -&gt; the next value, or raise StopIteration</p>
</dd></dl>

</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="#">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Hadoopy: Python wrapper for Hadoop using Cython</a><ul>
<li><a class="reference internal" href="#about">About</a></li>
<li><a class="reference internal" href="#benchmark">Benchmark</a></li>
<li><a class="reference internal" href="#example-hello-wordcount">Example - Hello Wordcount!</a></li>
<li><a class="reference internal" href="#api">API</a></li>
</ul>
</li>
</ul>

  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/index.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" size="18" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li><a href="#">Hadoopy v.01 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2011, Brandyn White.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.0.7.
    </div>
  </body>
</html>